import asyncio
import logging
from typing import Any, Dict, List, Optional, Tuple

from app.core.llm_service import LLMService

logger = logging.getLogger(__name__)


class CritiquerAgent:
    """
    Agent responsible for critiquing generated drafts and selecting the best one.
    """

    def __init__(self, llm_service: LLMService, temperature: float = 0.2):
        """
        Initializes the CritiquerAgent.

        Args:
            llm_service: The LLM service instance for generation.
            temperature: Low temperature suitable for analytical tasks.
        """
        self.llm_service = llm_service
        self.temperature = temperature
        logger.info(f"CritiquerAgent initialized with LLM Temp: {self.temperature}")

    async def critique_and_select(
        self,
        query: str,
        drafts: List[str],
        retrieved_context: List[Dict[str, Any]],
        chat_history: Optional[List[Dict]] = None,  # Optional context
    ) -> Tuple[List[str], int]:
        """
        Critiques each draft against the query and context, then selects the best index.

        Args:
            query: The user's query.
            drafts: A list of draft answers generated by the DraftingAgent.
            retrieved_context: The context used to generate the drafts.
            chat_history: Optional recent chat history.

        Returns:
            A tuple containing:
            - A list of critique strings, one for each draft.
            - The index (0-based) of the selected best draft.
            Returns ([], 0) on failure.
        """
        logger.info(
            f"CritiquerAgent critiquing {len(drafts)} drafts for query: '{query[:50]}...'"
        )

        if not self.llm_service:
            logger.error("LLM Service not available to CritiquerAgent.")
            return [], 0

        if not drafts:
            logger.warning("No drafts provided to CritiquerAgent.")
            return [], 0

        # --- Prepare Context and History Strings (similar to Drafter) ---
        context_strings = []
        for i, ctx in enumerate(retrieved_context):
            # Using the explicit concatenation style for maximum clarity and robustness
            filename = ctx.get("metadata", {}).get("filename", "Unknown")
            file_info = f"File: {filename}"  # Score is not typically used in critique context display
            source_header = f"Source {i+1} ({file_info}):"
            text_content = f"{ctx.get('text', '')}"  # Ensure text is fetched
            context_strings.append(
                f"{source_header}\n{text_content}"  # Explicitly add newline between header and its text
            )

        # Join with a clear separator between sources, similar to drafter.py
        context_str = "\n---\n".join(context_strings)

        if not context_str:  # Check after attempting to build
            context_str = "No context documents provided."

        history_str = ""
        if chat_history:
            history_str = "\n".join(
                [
                    f"{msg.get('role', 'unknown')}: {msg.get('content', '')}"
                    for msg in chat_history[-4:]
                ]
            )
            history_str = f"\n\nRecent Conversation History:\n{history_str}"

        # --- Construct Critique Prompt ---
        drafts_formatted = ""
        for i, draft in enumerate(drafts):
            drafts_formatted += f"--- Draft {i+1} ---\n{draft}\n\n"

        prompt = f"""
User Query: {query}
{history_str}

Context Documents Provided:
---
{context_str}
---

Generated Draft Answers:
{drafts_formatted}
Task: Critique each draft answer based on the following criteria:
1.  **Relevance:** Does the draft directly answer the User Query?
2.  **Completeness:** Does the draft address all aspects of the query based on the Context?
3.  **Accuracy/Faithfulness:** Does the draft accurately reflect the information in the Context Documents and avoid making things up?
4.  **Clarity:** Is the draft clear, concise, and easy to understand?

Provide a short critique for each draft, highlighting strengths and weaknesses based *only* on the criteria above and the provided information. Then, state which draft is the best overall, considering all criteria.

Critique Output Format:
Critique 1: [Your critique for Draft 1]
Critique 2: [Your critique for Draft 2]
Critique 3: [Your critique for Draft 3]
Best Draft Index: [Index number of the best draft, e.g., 1, 2, or 3]
"""

        # --- Generate Critiques using LLM ---
        generated_critique = ""
        try:
            logger.debug(
                f"Sending critique prompt to LLM (length: {len(prompt)} chars)"
            )
            generated_critique = await self.llm_service.generate(
                prompt=prompt,
                temperature=self.temperature,
                # max_tokens=500 # Limit critique length if needed
            )
            logger.debug(f"LLM generated critique length: {len(generated_critique)}")

        except Exception as e:
            logger.error(f"Error generating critiques from LLM: {e}", exc_info=True)
            # Fallback: Return empty critiques and select draft 0
            return [], 0

        # --- Parse Critiques and Selection ---
        critiques = []
        best_draft_index = 0  # Default to first draft
        try:
            critique_lines = generated_critique.strip().split("\n")
            parsed_critiques = {}
            current_critique_num = None

            for line in critique_lines:
                line_strip = line.strip()
                if line_strip.lower().startswith("critique "):
                    try:
                        num = int(line_strip.split()[1].strip(":"))
                        current_critique_num = num
                        critique_text = line_strip.split(":", 1)[1].strip()
                        parsed_critiques[current_critique_num] = critique_text
                    except (IndexError, ValueError):
                        logger.warning(
                            f"Could not parse critique number from line: {line_strip}"
                        )
                        current_critique_num = None  # Reset if parsing fails
                elif (
                    current_critique_num is not None
                    and current_critique_num in parsed_critiques
                ):
                    # Append continuation lines to the current critique
                    parsed_critiques[current_critique_num] += "\n" + line_strip
                elif line_strip.lower().startswith("best draft index:"):
                    try:
                        index_str = line_strip.split(":", 1)[1].strip()
                        # Try to convert potential number words or clean up string
                        if index_str.lower() == "one":
                            best_draft_index = 0
                        elif index_str.lower() == "two":
                            best_draft_index = 1
                        elif index_str.lower() == "three":
                            best_draft_index = 2
                        else:
                            best_draft_index = (
                                int(index_str) - 1
                            )  # Convert 1-based index to 0-based

                        # Clamp index to valid range
                        if not (0 <= best_draft_index < len(drafts)):
                            logger.warning(
                                f"LLM selected invalid best draft index {best_draft_index + 1}. Defaulting to 0."
                            )
                            best_draft_index = 0
                    except (IndexError, ValueError) as parse_err:
                        logger.warning(
                            f"Could not parse best draft index from line '{line_strip}': {parse_err}. Defaulting to 0."
                        )
                        best_draft_index = 0

            # Ensure critiques list matches the number of drafts
            critiques = [
                parsed_critiques.get(i + 1, "Critique parsing failed.")
                for i in range(len(drafts))
            ]

        except Exception as e:
            logger.error(f"Error parsing generated critiques: {e}", exc_info=True)
            logger.debug(
                f"Failed parsing LLM Raw Output for Critique:\n{generated_critique}"
            )
            # Fallback: return empty critiques, default selection
            return [], 0

        logger.info(
            f"Critique generated. Selected best draft index: {best_draft_index}"
        )
        return critiques, best_draft_index
